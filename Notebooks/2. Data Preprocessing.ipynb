{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Data Preprocessing.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install sastrawi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0qsZLtgyBtt","executionInfo":{"status":"ok","timestamp":1656306613880,"user_tz":-420,"elapsed":4778,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}},"outputId":"0e5f0a85-cf2b-4a7c-9d1b-6cceb70bd550"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sastrawi\n","  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 5.0 MB/s \n","\u001b[?25hInstalling collected packages: sastrawi\n","Successfully installed sastrawi-1.0.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","from tqdm import tqdm"],"metadata":{"id":"f7khnevfzO6f","executionInfo":{"status":"ok","timestamp":1656306620237,"user_tz":-420,"elapsed":2,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1XlLof4ux3QN","executionInfo":{"status":"ok","timestamp":1649655797226,"user_tz":-420,"elapsed":19810,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}},"outputId":"fc69d14e-7657-453c-f4ab-e99d2e8ad1a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["# imported from iFest 2021 Data Cleaning Module by Yaudahlah Teams,\n","# Refactored by Kaenova Mahendra Auditama (Yaudahlah Teams)\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import re\n","import string\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","\n","class DataCleaning:\n","  def __init__(self, stopword:list = [], slang_word:dict = {}) -> None:\n","    factory     = StemmerFactory()\n","    self.stemmer     = factory.create_stemmer()\n","    self.stopword = stopword\n","    self.slang_word = slang_word\n","\n","  def AddKamusAlay(self, new_dict:dict = {}):\n","    if (type(new_dict) != dict): raise TypeError(\"Not a valid type\")\n","    self.slang_word = self.slang_word | new_dict\n","  \n","  def AddStopWord(self, stopword:list = []):\n","    if (type(stopword) != list): raise TypeError(\"Not a valid type\")\n","    self.custom_word = self.custom_word + stopword\n","    \n","  def CleanDataFrame(self, df:pd.DataFrame, text_cols:str, label_cols:str, \n","                     word_min:int=0, label_mapping:dict=None, dropna:bool=False, verbose=False):\n","    \"\"\"\n","    Using multiprocessing (*if available) to process data from pandas Dataframe.\n","    Will be outputing a new dataframe with a processed data.\n","    \"\"\"\n","    print(\"Processing...\")\n","    final_list_clean = []\n","    final_list_dirty = []\n","    final_label = []\n","    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n","      sentence = row[text_cols]\n","      label = row[label_cols]\n","      \n","      # Process label\n","      if label_mapping is not None:\n","        if label not in label_mapping:\n","          print(f\"Label {label} is not matched any label_mapping you've defined. This label will be ignored\")\n","          continue      \n","        clean_label = label_mapping[label]\n","      else:\n","        clean_label = label  \n","      \n","      # Process Text\n","      clean_sentence = self.__cleanText__(sentence, self.slang_word,\n","                                          self.stopword, self.stemmer, verbose)\n","      if (clean_sentence is None):\n","        print(f\"Sentence '{sentence}' is empty after processing. This sentence will be ignored\")\n","        continue\n","      if (len(clean_sentence.split()) < word_min):\n","        continue\n","      \n","      final_list_clean.append(clean_sentence)\n","      final_list_dirty.append(sentence)\n","      final_label.append(clean_label)\n","        \n","    # Creating pandas dataframe\n","    data = {\n","      'raw': final_list_dirty,\n","      'processed': final_list_clean,\n","      'label': final_label\n","    }\n","    final_df = pd.DataFrame(data)\n","    if dropna:\n","      print(\"NaN Dropped\")\n","      final_df = final_df.dropna(how='any')\n","    final_df['processed'] = final_df['processed'].astype(str)\n","    final_df['raw'] = final_df['raw'].astype(str)\n","\n","    return final_df\n","\n","  def CleanOneText(self, text, verbose=False):\n","    return self.__cleanText__(text, self.slang_word, self.stopword, self.stemmer, verbose)\n","\n","  def __cleanText__(self, text:str, slangword:dict, stopword:list, stemmer, verbose=False) -> str:\n","    '''\n","    Processing a text, deleting some web associated word, removing word from stopword list\n","    and change defined slang word.\n","    '''\n","    # HTML and text annotation removal\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub('(@\\w+|#\\w+)','',text)\n","    text = re.sub('<.*?>', '', text)  \n","    temp_text = list(text)\n","    for i in range(len(temp_text)):\n","      if temp_text[i] in string.punctuation:\n","        temp_text[i] = \" \"\n","    text = ''.join(temp_text)\n","    text = re.sub('[^a-zA-Z]',' ',text) \n","    text = re.sub(\"\\n\",\" \",text)\n","    text = text.lower()\n","    text = re.sub(\"(username|user|url|rt|xf|fx|xe|xa)\\s|\\s(user|url|rt|xf|fx|xe|xa)\",\"\",text)\n","    text = re.sub(r'(\\w)(\\1{2,})', r\"\\1\", text)\n","    text = re.sub(r\"\\b[a-zA-Z]\\b\",\"\",text)\n","    text = re.sub('(s{2,})',' ',text)\n","    if verbose:\n","      print(f\"After Special Character : {text}\")\n","    text = text.lower()\n","    if verbose:\n","      print(f\"After Lower : {text}\")\n","    text=' '.join(text.split())\n","    text_split = text.split(' ')\n","    final_text_split = []\n","    for i in range(len(text_split)):\n","      if type(text_split[i]) != str:\n","        continue\n","      if str(text_split[i]) in stopword:\n","        continue\n","      if str(text_split[i]) in slangword:\n","        text_split[i] = str(slangword[text_split[i]])\n","      final_text_split.append(text_split[i])\n","    text = \" \".join(final_text_split)\n","    if verbose:\n","      print(f\"After Stopword and Slangword : {text}\")\n","    stemmed_text = stemmer.stem(text)\n","    if verbose:\n","      print(f\"After Stemming : {stemmed_text}\")\n","    \n","    # just to make sure\n","    if len(stemmed_text) == 0:\n","      return None   \n","    \n","    return stemmed_text"],"metadata":{"id":"4zdg865RyK0E","executionInfo":{"status":"ok","timestamp":1656307319802,"user_tz":-420,"elapsed":531,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Preparing Slang Word"],"metadata":{"id":"vm1uGBiqygLC"}},{"cell_type":"code","source":["kamus_alay1 = pd.read_csv('https://raw.githubusercontent.com/fendiirfan/Kamus-Alay/main/Kamu-Alay.csv')\n","dict_kamus_alay1 = {}\n","for _,row in kamus_alay1.iterrows():\n","    if row[\"kataBaik\"] is np.NaN:\n","        continue\n","    dict_kamus_alay1[row[\"kataAlay\"]] = row[\"kataBaik\"]"],"metadata":{"id":"wurbL-Juyf9l","executionInfo":{"status":"ok","timestamp":1656306630564,"user_tz":-420,"elapsed":944,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["kamus_alay2 = pd.read_csv('https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv')\n","kamus_alay2 = kamus_alay2.filter(['slang', 'formal'], axis=1)\n","kamus_alay2 = kamus_alay2.drop_duplicates(subset=['slang'], keep='first')\n","dict_kamus_alay2 = {}\n","for _,row in kamus_alay2.iterrows():\n","    if row[\"formal\"] is np.NaN:\n","        continue\n","    dict_kamus_alay2[row[\"slang\"]] = row[\"formal\"]"],"metadata":{"id":"gRhUESW2yeDH","executionInfo":{"status":"ok","timestamp":1656306631826,"user_tz":-420,"elapsed":1269,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Instantiate Cleaner"],"metadata":{"id":"a4syuvBuynC9"}},{"cell_type":"code","source":["cleaner = DataCleaning([], {**dict_kamus_alay1, **dict_kamus_alay2})"],"metadata":{"id":"YoCO0_SZyk1O","executionInfo":{"status":"ok","timestamp":1656307322614,"user_tz":-420,"elapsed":309,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Script for cleaning text Twitter"],"metadata":{"id":"4m9bNTjEzHQR"}},{"cell_type":"code","source":["RAW_DIR = \"/content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Crawling/RAW\"\n","PROCESSED_DIR = \"/content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Crawling/Processed\""],"metadata":{"id":"RGjsjeqK93QO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_files_dir(cleaner: DataCleaning, src_dir:str, dst_dir:str,\n","                    text_column: str):\n","    # Removing files in destination if exists\n","    files = os.listdir(dst_dir)\n","    for file in files:\n","        os.remove(dst_dir + f\"/{file}\")\n","\n","    # Cleaning files\n","    files = os.listdir(src_dir)\n","    for file in files:\n","        print(f\"Processing file {file}\")\n","        df = pd.read_csv(src_dir+f\"/{file}\")\n","        unclean_texts = []\n","        clean_texts = []\n","        for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n","            text = row[text_column]\n","            unclean_texts.append(text)\n","            text = cleaner.CleanOneText(str(text))\n","            clean_texts.append(text)\n","        new_df = pd.DataFrame(data={\"raw\" : unclean_texts, \"clean\" : clean_texts})\n","        new_df = new_df.drop_duplicates()\n","        new_df = new_df.dropna()\n","        new_df.to_csv(dst_dir+f\"/{file}\", index=False)\n","\n","clean_files_dir(cleaner, RAW_DIR, PROCESSED_DIR, \"tweet\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbZMKtH6yxKd","executionInfo":{"status":"ok","timestamp":1649067704290,"user_tz":-420,"elapsed":3742899,"user":{"displayName":"Kaenova Mahendra Auditama","userId":"02421881994816586492"}},"outputId":"17afe329-e9a8-49fe-e746-d5d1650fe12a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file sinopharm.csv\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2149/2149 [00:42<00:00, 50.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing file moderna.csv\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4693/4693 [07:28<00:00, 10.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing file efektif.csv\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10300/10300 [08:05<00:00, 21.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing file pfizer.csv\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9972/9972 [10:18<00:00, 16.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing file bayar.csv\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1694/1694 [04:11<00:00,  6.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing file gratis.csv\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 11277/11277 [11:31<00:00, 16.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing file sinovac.csv\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21663/21663 [16:29<00:00, 21.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing file astrazeneca.csv\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9103/9103 [03:30<00:00, 43.17it/s]\n"]}]},{"cell_type":"markdown","source":["# Script for cleaning text General Sentiment Analysis"],"metadata":{"id":"obeSOIGh9whn"}},{"cell_type":"code","source":["label_mapping = {\n","    -1 : 0, # Negatively sentiment\n","    0 : 1, # Neutral sentiment\n","    1 : 2 # Positive sentiment\n","}\n"],"metadata":{"id":"Uu0LD9nu95hJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# IndoNLU\n","df = pd.read_csv(\"/content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/1. RAW/General Sentiment Analysis Labeled/IndoNLU_SMSA_DOC-SENTIMENT_PROSA.csv\")\n","unclean_texts = []\n","clean_texts = []\n","labels = []\n","for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n","  text = str(row[\"tweet\"])\n","  unclean_texts.append(text)\n","  text = cleaner.CleanOneText(str(text))\n","  clean_texts.append(text)\n","  labels.append(row[\"labels\"])\n","new_df = pd.DataFrame(data={\"raw\" : unclean_texts, \"clean\" : clean_texts, \"labels\": labels})\n","new_df = new_df.drop_duplicates()\n","new_df = new_df.dropna()\n","new_df.to_csv(\"/content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/General/IndoNLU.csv\", index=False)"],"metadata":{"id":"JEt273hd-d_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ridlife\n","df = pd.read_csv(\"/content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/1. RAW/General Sentiment Analysis Labeled/ridlife-dataset-idsa-Indonesian Sentiment Twitter Dataset Labeled.csv\")\n","unclean_texts = []\n","clean_texts = []\n","labels = []\n","for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n","  text = str(row[\"Tweet\"])\n","  unclean_texts.append(text)\n","  text = cleaner.CleanOneText(str(text))\n","  clean_texts.append(text)\n","  labels.append(row[\"sentimen\"])\n","new_df = pd.DataFrame(data={\"raw\" : unclean_texts, \"clean\" : clean_texts, \"labels\": labels})\n","new_df = new_df.drop_duplicates()\n","new_df = new_df.dropna()\n","new_df.to_csv(\"/content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/General/Ridlife.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZV1C6Zb_Jnt","executionInfo":{"status":"ok","timestamp":1649638585177,"user_tz":-420,"elapsed":1384495,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}},"outputId":"c0ee34d2-b869-4808-ba2b-fff9d46781b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10806/10806 [23:03<00:00,  7.81it/s]\n"]}]},{"cell_type":"markdown","source":["Improving Cleaning Data   \n","This needs to be done because there's some duplicates on \"clean\" column. This gives us insight that there's some tweet that have same context. Like a newsline and spam. Hence, we need to remove that and keep it only the first one. This can be seen on the \"clean data moderna v1\""],"metadata":{"id":"ywAJUZiDmwYQ"}},{"cell_type":"code","source":["import mimetypes\n","import os"],"metadata":{"id":"b4rrw_supvdY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We are going to clean all the duplicates on \"clean\" column on every dataset\n","\n","input_path = \"/content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/Processed/v2\"\n","based_column_clean = \"clean\"\n","\n","def clean_duplicates(input_dir:str, column:str=None) -> None:\n","  new_input_dir = input_dir+\"/improved/\"\n","  if not os.path.exists(new_input_dir):\n","    os.mkdir(new_input_dir)\n","  list_file = os.listdir(input_dir)\n","  for i in list_file:\n","    cur_file = input_dir+f\"/{i}\"\n","    if mimetypes.guess_type(cur_file)[0] != \"text/csv\":\n","      continue\n","    df = pd.read_csv(cur_file)\n","    df = df.drop_duplicates(subset=column)\n","    output_dir = new_input_dir+f\"{i}\"\n","    df.to_csv(output_dir, index=False)\n","    print(f\"{i} has been cleaned and outputted to {output_dir}\")\n","\n","clean_duplicates(input_path, based_column_clean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WbKQn_MfmyeB","executionInfo":{"status":"ok","timestamp":1649631198812,"user_tz":-420,"elapsed":2239,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}},"outputId":"bb6c02d8-b5c0-4cdb-a49d-1f04043b063b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sinopharm.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/Processed/v2/improved/sinopharm.csv\n","moderna.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/Processed/v2/improved/moderna.csv\n","efektif.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/Processed/v2/improved/efektif.csv\n","pfizer.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/Processed/v2/improved/pfizer.csv\n","bayar.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/Processed/v2/improved/bayar.csv\n","gratis.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/Processed/v2/improved/gratis.csv\n","sinovac.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/Processed/v2/improved/sinovac.csv\n","astrazeneca.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/Processed/v2/improved/astrazeneca.csv\n"]}]},{"cell_type":"markdown","source":["Let's clean up tweet with the links  \n","Because link tend to direct to a news"],"metadata":{"id":"8eJ6LXLYJKbz"}},{"cell_type":"code","source":["# We are going to clean all the duplicates on \"clean\" column on every dataset\n","# reference: https://stackoverflow.com/questions/39948757/how-to-delete-rows-in-python-pandas-dataframe-using-regular-expressions\n","import os\n","import mimetypes\n","\n","input_path = \"/content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v2/improved\"\n","output_path = \"/content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v3\"\n","\n","def remove_row_with_links(input_dir:str, output_path) -> None:\n","  if not os.path.exists(output_path):\n","    os.mkdir(output_path)\n","  list_file = os.listdir(input_dir)\n","  for i in list_file:\n","    cur_file = input_dir+f\"/{i}\"\n","    if mimetypes.guess_type(cur_file)[0] != \"text/csv\":\n","      continue\n","    df = pd.read_csv(cur_file)\n","    patternDel = \"https://t.co\" # Beccause so many news with this links\n","    filter = df['raw'].str.contains(patternDel)\n","    df = df[~filter].reset_index(drop=True)\n","    output_dir = output_path+f\"/{i}\"\n","    df.to_csv(output_dir, index=False)\n","    print(f\"{i} has been cleaned and outputted to {output_dir}\")\n","\n","remove_row_with_links(input_path, output_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NrxNi9C_JRN0","executionInfo":{"status":"ok","timestamp":1649656339546,"user_tz":-420,"elapsed":1760,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}},"outputId":"f9476527-7594-4f90-b549-5eaac160b882"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sinopharm.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v3/sinopharm.csv\n","bayar.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v3/bayar.csv\n","pfizer.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v3/pfizer.csv\n","moderna.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v3/moderna.csv\n","efektif.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v3/efektif.csv\n","gratis.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v3/gratis.csv\n","astrazeneca.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v3/astrazeneca.csv\n","sinovac.csv has been cleaned and outputted to /content/gdrive/Shareddrives/Riset Sentimen Vaksin COVID: Yaudahlah/Riset/Data/2. Clean/Twitter/v3/sinovac.csv\n"]}]},{"cell_type":"markdown","source":["# Tahapan Preprocessing"],"metadata":{"id":"-vwMmhn7lOLe"}},{"cell_type":"code","source":["text = \"Vaksin ma perawatan covid itu gratis. Pemerintah untung dari mana coba?\""],"metadata":{"id":"gG8cHOJKlLaJ","executionInfo":{"status":"ok","timestamp":1656307327009,"user_tz":-420,"elapsed":309,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["cleaner.CleanOneText(text, True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"tEYjcm9klrnW","executionInfo":{"status":"ok","timestamp":1656307327729,"user_tz":-420,"elapsed":404,"user":{"displayName":"Kaenova Mahendra","userId":"02905267376027133872"}},"outputId":"3f61050f-c666-4c90-86b9-be2fa55396c1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["After Special Character : vaksin ma perawatan covid itu gratis  pemerintah untung dari mana coba \n","After Lower : vaksin ma perawatan covid itu gratis  pemerintah untung dari mana coba \n","After Stopword and Slangword : vaksin sama perawatan covid itu gratis pemerintah untung dari mana coba\n","After Stemming : vaksin sama awat covid itu gratis perintah untung dari mana coba\n"]},{"output_type":"execute_result","data":{"text/plain":["'vaksin sama awat covid itu gratis perintah untung dari mana coba'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":[""],"metadata":{"id":"0h8WGfZWmkt-"},"execution_count":null,"outputs":[]}]}